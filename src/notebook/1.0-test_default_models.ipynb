{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375420b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Open3D not available. 3D interactive visualization will be disabled.\n",
      "To enable Open3D visualization, activate the py312 environment:\n",
      "  conda activate py312\n",
      "  python scripts/nuscenes.py [your_arguments]\n",
      "âœ… Successfully imported nuscenes utilities\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "PROJECT_NAME = \"customRCNN\"\n",
    "PROJECT_DIR = os.path.join(os.path.abspath('.').split(PROJECT_NAME)[0], PROJECT_NAME)\n",
    "DETECTION_DIR = os.path.join(PROJECT_DIR, \"DeepDataMiningLearning\", \"detection\")\n",
    "\n",
    "sys.path.insert(0, DETECTION_DIR)\n",
    "sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "from dataset_nuscenescoco import NuscenesCOCODataset\n",
    "from dataset_nuscenes import create_nuscenes_transforms\n",
    "from dataset_waymococo import WaymoCOCODataset, get_transformsimple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c029f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUSCENES_CLASS_NAMES = {\n",
    "    0: 'car',\n",
    "    1: 'truck',\n",
    "    2: 'bus',\n",
    "    3: 'trailer',\n",
    "    4: 'construction_vehicle',\n",
    "    5: 'pedestrian',\n",
    "    6: 'motorcycle',\n",
    "    7: 'bicycle',\n",
    "    8: 'traffic_cone',\n",
    "    9: 'barrier',\n",
    "}\n",
    "\n",
    "WAYMO_CLASS_NAMES = {\n",
    "    1: \"Vehicle\",\n",
    "    2: \"Pedestrian\",\n",
    "    3: \"Cyclist\",\n",
    "    4: \"Sign\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db149d2a",
   "metadata": {},
   "source": [
    "## 1. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc0c7c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.13s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.49s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def get_nuscenes_dataset():\n",
    "    transform = create_nuscenes_transforms(train=True)\n",
    "    dataset = NuscenesCOCODataset(\n",
    "        root=f'{PROJECT_DIR}/data/nuscenes_subset_coco_step10',\n",
    "        annotation = f'{PROJECT_DIR}/data/nuscenes_subset_coco_step10/annotations.json',\n",
    "        train=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_waymo_dataset():\n",
    "    transform = get_transformsimple(None)\n",
    "    dataset = WaymoCOCODataset(\n",
    "        root=f'{PROJECT_DIR}/data/waymo',\n",
    "        annotation = f'{PROJECT_DIR}/data/waymo/annotations.json',\n",
    "        train=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "nd = get_nuscenes_dataset()\n",
    "wd = get_waymo_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce26e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '__background__',\n",
       " 1: 'Vehicles',\n",
       " 2: 'Pedestrians',\n",
       " 3: 'Cyclists',\n",
       " 4: 'Signs'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = {i: value for i, value in enumerate(wd.INSTANCE_CATEGORY_NAMES)}\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78deecc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size = torch.Size([3, 900, 1600])\n",
      "image sample = tensor([2.2489, 2.2489, 2.2489, 2.2489, 2.2489, 2.2489, 2.2489, 2.2489, 2.2489,\n",
      "        2.2489])\n"
     ]
    }
   ],
   "source": [
    "images, targets = nd[0]\n",
    "print(f\"image size = {images.size()}\")  # [C, H, W]\n",
    "print(f\"image sample = {images[0][0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31ca0e",
   "metadata": {},
   "source": [
    "## 2. load baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd390f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torchvision detection models: ['fasterrcnn_mobilenet_v3_large_320_fpn', 'fasterrcnn_mobilenet_v3_large_fpn', 'fasterrcnn_resnet50_fpn', 'fasterrcnn_resnet50_fpn_v2', 'fcos_resnet50_fpn', 'keypointrcnn_resnet50_fpn', 'maskrcnn_resnet50_fpn', 'maskrcnn_resnet50_fpn_v2', 'retinanet_resnet50_fpn', 'retinanet_resnet50_fpn_v2', 'ssd300_vgg16', 'ssdlite320_mobilenet_v3_large']\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models import get_model, get_model_weights, get_weight, list_models\n",
    "from src.modeling.modeling_rpnfasterrcnn import CustomRCNN\n",
    "\n",
    "detectionmodel_names=list_models(module=torchvision.models.detection)\n",
    "print(\"Torchvision detection models:\", detectionmodel_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0605ae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('boxes', torch.Size([100, 4])), ('labels', torch.Size([100])), ('scores', torch.Size([100]))]\n"
     ]
    }
   ],
   "source": [
    "backbonename='resnet50'\n",
    "trainable_layers =2\n",
    "#layers_to_train = [\"layer4\", \"layer3\", \"layer2\", \"layer1\", \"conv1\"]\n",
    "num_classes = 10\n",
    "model=CustomRCNN(backbone_modulename=backbonename,trainable_layers=trainable_layers,num_classes=num_classes, out_channels=256, min_size=800, max_size=1333)\n",
    "model.eval()\n",
    "\n",
    "images, targets = nd[0]\n",
    "output = model([images], [targets])\n",
    "\n",
    "print([(k, v.shape) for k, v in output[0].items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4b2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 5, 4, 4, 4, 2, 4, 4, 5, 4, 5, 8, 4, 4, 4, 4, 4, 4, 5, 5, 7,\n",
       "        4, 5, 4, 4, 5, 4, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 2, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 8, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 3, 5, 4, 4, 4, 4, 8, 4,\n",
       "        4, 4, 4, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample output\n",
    "box = output[0]['boxes'][0]\n",
    "label = output[0]['labels'][0]\n",
    "score = output[0]['scores'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c2433ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
       "        0.9999, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9997, 0.9997, 0.9997,\n",
       "        0.9996, 0.9995, 0.9995, 0.9994, 0.9993, 0.9993, 0.9992, 0.9992, 0.9990,\n",
       "        0.9990, 0.9989, 0.9988, 0.9984, 0.9982, 0.9982, 0.9982, 0.9979, 0.9975,\n",
       "        0.9973, 0.9972, 0.9972, 0.9972, 0.9971, 0.9969, 0.9968, 0.9965, 0.9963,\n",
       "        0.9958, 0.9958, 0.9956, 0.9956, 0.9951, 0.9944, 0.9941, 0.9939, 0.9935,\n",
       "        0.9935, 0.9932, 0.9932, 0.9930, 0.9918, 0.9912, 0.9910, 0.9906, 0.9899,\n",
       "        0.9898], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a02e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
